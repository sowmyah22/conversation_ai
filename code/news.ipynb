{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converation AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 18:33:51,884 - INFO - Loaded 2 keywords: ['city', 'bangalore']\n",
      "2025-05-01 18:33:59,935 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://timesofindia.indiatimes.com/rss\n",
      "2025-05-01 18:34:00,099 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://timesofindia.indiatimes.com/feeds\n",
      "2025-05-01 18:34:00,201 - CRITICAL - [REQUEST FAILED] 404 Client Error: Not Found for url: https://timesofindia.indiatimes.com/feed\n",
      "2025-05-01 18:34:01,713 - INFO - Found 329 articles to process\n",
      "2025-05-01 18:34:03,432 - WARNING - Error processing article https://timesascent.com/articles/india-inc-races-to-bridge-ai-skill-gap-as-demand-outpaces-supply: unsupported operand type(s) for +: 'NoneType' and 'str'\n",
      "2025-05-01 18:34:04,197 - INFO - Processed article 10: Infosys Eyes Experienced Tech Workers across 40+ Skill Sets - Keywords: ['city']\n",
      "2025-05-01 18:34:06,349 - INFO - Processed article 20: Instagram’s head Adam Mosseri dances with influencers, gets a Bollywood welcome from Ranveer and Deepika in Mumbai - Keywords: ['city']\n",
      "2025-05-01 18:34:06,687 - INFO - Processed article 24: Yolanthe OTT Release Date: When to watch Dutch reality show peeking into Yolanthe Cabau’s life - Keywords: ['city']\n",
      "2025-05-01 18:34:07,629 - INFO - Processed article 30: Met Gala 2025: Meet Anna Wintour, Vogue’s editor-in-chief and fashion’s most powerful force - Keywords: ['city']\n",
      "2025-05-01 18:34:08,892 - INFO - Processed article 38: Hania Aamir’s ‘message to PM Modi’ over Instagram ban: Is she blaming Pakistani army for Pahalgam attack? - Keywords: ['city']\n",
      "2025-05-01 18:34:09,239 - INFO - Processed article 41: Another global pandemic fears rise as H5N1 bird flu spreads across US farms - Keywords: ['city']\n",
      "2025-05-01 18:34:09,872 - INFO - Processed article 45: Rohit Sharma's net worth: Inside Hitman's net worth, income, IPL & BCCI salary, house, cars on his 38th birthday - Keywords: ['city']\n",
      "2025-05-01 18:34:10,268 - INFO - Processed article 46: Mark Carney's net worth: How rich is the 24th PM of Canada? A look at his career, family - Keywords: ['city']\n",
      "2025-05-01 18:34:32,926 - INFO - Processed article 198: Smartphone Under 10K: Mobile Phones under 10000 - Keywords: ['city']\n",
      "2025-05-01 18:34:33,931 - INFO - Processed article 199: Smartphones under 20K: Mobile Phones under 20000 - Keywords: ['city']\n",
      "2025-05-01 18:34:36,331 - INFO - Processed article 205: Laptops under 40000 Online in India – Gadgets Now - Keywords: ['city']\n",
      "2025-05-01 18:34:48,353 - INFO - Processed article 235: AC Under 30000 Online in India - Keywords: ['city']\n",
      "2025-05-01 18:34:51,330 - INFO - Processed article 246: Best Refrigerators under 10000 Online in India - Keywords: ['city']\n",
      "2025-05-01 18:35:11,799 - INFO - Saved 13 articles to toi_articles_all.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Saved 13 articles to toi_articles_all.json\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Read keywords from text file\n",
    "try:\n",
    "    with open('keywords.txt', 'r', encoding='utf-8') as f:\n",
    "        keywords = [line.strip() for line in f if line.strip()]\n",
    "    logger.info(f\"Loaded {len(keywords)} keywords: {keywords}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"keywords.txt not found. Please create the file with one keyword per line.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading keywords.txt: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize Times of India newspaper object\n",
    "try:\n",
    "    toi = newspaper.build('https://timesofindia.indiatimes.com/', \n",
    "                         memoize_articles=False, \n",
    "                         fetch_images=False, \n",
    "                         number_threads=4)\n",
    "    logger.info(f\"Found {len(toi.articles)} articles to process\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize newspaper: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Function to process a single article\n",
    "def process_article(article, index):\n",
    "    try:\n",
    "        # Download and parse article\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        # Check if any keyword is in the article title or text\n",
    "        article_text = (article.title + \" \" + article.text).lower()\n",
    "        matched_keywords = [kw for kw in keywords if kw in article_text]\n",
    "        \n",
    "        if matched_keywords:\n",
    "            # Extract relevant data\n",
    "            article_info = {\n",
    "                'title': article.title,\n",
    "                'url': article.url,\n",
    "                'publish_date': str(article.publish_date) if article.publish_date else None,\n",
    "                'keywords': matched_keywords,\n",
    "                'summary': article.text[:500] + \"...\" if len(article.text) > 500 else article.text\n",
    "            }\n",
    "            return article_info, index\n",
    "        return None, index\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error processing article {article.url}: {e}\")\n",
    "        return None, index\n",
    "\n",
    "# Process articles in parallel\n",
    "max_workers = 8  # Adjust based on system and server tolerance\n",
    "articles_data = []\n",
    "try:\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all article processing tasks\n",
    "        future_to_article = {executor.submit(process_article, article, i): article \n",
    "                            for i, article in enumerate(toi.articles, 1)}\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_article):\n",
    "            article_info, index = future.result()\n",
    "            if article_info:\n",
    "                articles_data.append(article_info)\n",
    "                logger.info(f\"Processed article {index}: {article_info['title']} - Keywords: {article_info['keywords']}\")\n",
    "            # Small delay to avoid overwhelming the server\n",
    "            time.sleep(0.1)  # 100ms delay between requests\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during parallel processing: {e}\")\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = 'toi_articles_all.json'\n",
    "try:\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles_data, f, indent=4, ensure_ascii=False)\n",
    "    logger.info(f\"Saved {len(articles_data)} articles to {output_file}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save JSON file: {e}\")\n",
    "\n",
    "print(f\"Scraping complete. Saved {len(articles_data)} articles to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "* Newspaper 3k only works for static html pages and can't handle heavy javascript rendered pages\n",
    "* using different library which could handle dynamic and heavy javascript rendered pagespages and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4109023801.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    playwright install\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!pip install playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample to test with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 14:58:58,545 - INFO - Loaded 2 keywords: ['city', 'business']\n",
      "2025-05-02 14:58:59,515 - ERROR - Error fetching articles for keyword city: BrowserType.launch: Executable doesn't exist at /Users/pranavi/Library/Caches/ms-playwright/chromium_headless_shell-1169/chrome-mac/headless_shell\n",
      "╔════════════════════════════════════════════════════════════╗\n",
      "║ Looks like Playwright was just installed or updated.       ║\n",
      "║ Please run the following command to download new browsers: ║\n",
      "║                                                            ║\n",
      "║     playwright install                                     ║\n",
      "║                                                            ║\n",
      "║ <3 Playwright Team                                         ║\n",
      "╚════════════════════════════════════════════════════════════╝\n",
      "2025-05-02 14:58:59,516 - WARNING - No articles found for keyword: city\n",
      "2025-05-02 14:59:00,088 - ERROR - Error fetching articles for keyword business: BrowserType.launch: Executable doesn't exist at /Users/pranavi/Library/Caches/ms-playwright/chromium_headless_shell-1169/chrome-mac/headless_shell\n",
      "╔════════════════════════════════════════════════════════════╗\n",
      "║ Looks like Playwright was just installed or updated.       ║\n",
      "║ Please run the following command to download new browsers: ║\n",
      "║                                                            ║\n",
      "║     playwright install                                     ║\n",
      "║                                                            ║\n",
      "║ <3 Playwright Team                                         ║\n",
      "╚════════════════════════════════════════════════════════════╝\n",
      "2025-05-02 14:59:00,091 - WARNING - No articles found for keyword: business\n",
      "2025-05-02 14:59:00,097 - INFO - Saved 0 articles to toi_articles_by_keyword.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Saved 0 articles to toi_articles_by_keyword.json\n"
     ]
    }
   ],
   "source": [
    "# For Jupyter notebooks compatibility\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    print(\"nest_asyncio not found. Install it with: pip install nest_asyncio\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Read keywords from text file\n",
    "keywords = ['city', 'business']  # Default keywords for demonstration\n",
    "try:\n",
    "    with open('keywords.txt', 'r', encoding='utf-8') as f:\n",
    "        keywords = [line.strip() for line in f if line.strip()]\n",
    "    logger.info(f\"Loaded {len(keywords)} keywords: {keywords}\")\n",
    "except FileNotFoundError:\n",
    "    logger.info(\"keywords.txt not found. Using default keywords: {keywords}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading keywords.txt: {e}\")\n",
    "    logger.info(f\"Using default keywords: {keywords}\")\n",
    "\n",
    "# Function to fetch article URLs from a keyword search page using Playwright Async API\n",
    "async def fetch_article_urls(keyword):\n",
    "    browser = None\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            # Launch headless browser\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "\n",
    "            # Construct the keyword search URL\n",
    "            base_url = 'https://timesofindia.indiatimes.com/topic/'\n",
    "            keyword_url = f\"{base_url}{quote(keyword)}\"\n",
    "            logger.info(f\"Fetching articles for keyword: {keyword} from {keyword_url}\")\n",
    "\n",
    "            # Navigate to the keyword page and wait for content to load\n",
    "            await page.goto(keyword_url, wait_until=\"domcontentloaded\")\n",
    "            # Wait for article links to appear\n",
    "            await page.wait_for_selector('a[href*=\"/articleshow/\"]', timeout=10000)\n",
    "\n",
    "            # Extract article links from rendered HTML\n",
    "            article_links = []\n",
    "            links = await page.query_selector_all('a[href*=\"/articleshow/\"]')\n",
    "            for link in links:\n",
    "                href = await link.get_attribute('href')\n",
    "                if href:\n",
    "                    # Convert relative URLs to absolute\n",
    "                    if href.startswith('/'):\n",
    "                        href = 'https://timesofindia.indiatimes.com' + href\n",
    "                    # Ensure unique links and valid article URLs\n",
    "                    if href not in article_links and re.match(r'.*/articleshow/\\d+\\.cms', href):\n",
    "                        article_links.append(href)\n",
    "\n",
    "            logger.info(f\"Found {len(article_links)} articles for keyword: {keyword}\")\n",
    "            return article_links, keyword\n",
    "    except asyncio.TimeoutError:\n",
    "        logger.error(f\"Timeout fetching articles for keyword {keyword}\")\n",
    "        return [], keyword\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching articles for keyword {keyword}: {e}\")\n",
    "        return [], keyword\n",
    "    finally:\n",
    "        if browser:\n",
    "            await browser.close()\n",
    "\n",
    "# Function to process a single article using Playwright Async API\n",
    "async def process_article(article_url, keyword, index):\n",
    "    browser = None\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            # Launch headless browser\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "\n",
    "            # Navigate to article page and wait for content to load\n",
    "            await page.goto(article_url, wait_until=\"domcontentloaded\")\n",
    "            # Wait for article content to appear\n",
    "            await page.wait_for_selector('h1, h2', timeout=10000)\n",
    "\n",
    "            # Extract title\n",
    "            title_elem = await page.query_selector('h1, h2')\n",
    "            title = await title_elem.inner_text() if title_elem else \"No title found\"\n",
    "            title = title.strip()\n",
    "\n",
    "            # Extract publish date\n",
    "            date_selector = 'div.as_byline > div > span, time, meta[name=\"publish-date\"]'\n",
    "            date_elem = await page.query_selector(date_selector)\n",
    "            publish_date = await date_elem.inner_text() if date_elem else None\n",
    "            if not publish_date and date_elem:\n",
    "                publish_date = await date_elem.get_attribute('content') or await date_elem.get_attribute('datetime')\n",
    "            if publish_date:\n",
    "                publish_date = publish_date.strip()\n",
    "\n",
    "            # Extract full article text\n",
    "            text_selector = 'div._s30W > div._3WlLe, div.articlebody, div.content'\n",
    "            article_body = await page.query_selector_all(text_selector)\n",
    "            article_text = ' '.join([await elem.inner_text() for elem in article_body]).strip() if article_body else \"\"\n",
    "\n",
    "            # Extract author\n",
    "            author_selector = 'div.as_byline > a, span.author, meta[name=\"author\"]'\n",
    "            author_elem = await page.query_selector(author_selector)\n",
    "            author = await author_elem.inner_text() if author_elem else None\n",
    "            if not author and author_elem:\n",
    "                author = await author_elem.get_attribute('content')\n",
    "            if author:\n",
    "                author = author.strip()\n",
    "\n",
    "            # Extract article-specific keywords/tags\n",
    "            article_keywords = []\n",
    "            # Try meta keywords\n",
    "            meta_keywords = await page.query_selector('meta[name=\"keywords\"]')\n",
    "            if meta_keywords:\n",
    "                keywords_content = await meta_keywords.get_attribute('content')\n",
    "                if keywords_content:\n",
    "                    article_keywords.extend([kw.strip() for kw in keywords_content.split(',')])\n",
    "            # Try visible tags\n",
    "            tag_elems = await page.query_selector_all('div.tags a, ul.tags li a')\n",
    "            if tag_elems:\n",
    "                article_keywords.extend([await elem.inner_text() for elem in tag_elems])\n",
    "            # Remove duplicates and empty strings\n",
    "            article_keywords = list(set([kw.strip() for kw in article_keywords if kw]))\n",
    "\n",
    "            # Create article info\n",
    "            article_info = {\n",
    "                'title': title,\n",
    "                'url': article_url,\n",
    "                'publish_date': publish_date,\n",
    "                'keyword': keyword,\n",
    "                'author': author,\n",
    "                'article_keywords': article_keywords,\n",
    "                'full_text': article_text\n",
    "            }\n",
    "            return article_info, index\n",
    "    except asyncio.TimeoutError:\n",
    "        logger.warning(f\"Timeout processing article {article_url}\")\n",
    "        return None, index\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error processing article {article_url}: {e}\")\n",
    "        return None, index\n",
    "    finally:\n",
    "        if browser:\n",
    "            await browser.close()\n",
    "\n",
    "# Main function to handle async operations\n",
    "async def main():\n",
    "    articles_data = []\n",
    "    max_workers = 3  # Reduced workers to minimize server load\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        # Fetch article URLs for the keyword\n",
    "        article_urls, kw = await fetch_article_urls(keyword)\n",
    "        \n",
    "        if not article_urls:\n",
    "            logger.warning(f\"No articles found for keyword: {keyword}\")\n",
    "            continue\n",
    "\n",
    "        # Process articles with concurrency control\n",
    "        tasks = []\n",
    "        for i, url in enumerate(article_urls[:10], 1):  # Limit to first 10 articles per keyword\n",
    "            # Add small delay between task creations to avoid overwhelming the server\n",
    "            if i > 1:\n",
    "                await asyncio.sleep(0.2)  # 200ms delay\n",
    "            tasks.append(process_article(url, keyword, i))\n",
    "        \n",
    "        # Process concurrently but with limited concurrency\n",
    "        for i in range(0, len(tasks), max_workers):\n",
    "            batch = tasks[i:i+max_workers]\n",
    "            results = await asyncio.gather(*batch)\n",
    "            for article_info, index in results:\n",
    "                if article_info:\n",
    "                    articles_data.append(article_info)\n",
    "                    logger.info(f\"Processed article {index} for keyword {keyword}: {article_info['title']}\")\n",
    "\n",
    "    # Save to JSON file\n",
    "    output_file = 'toi_articles_by_keyword.json'\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles_data, f, indent=4, ensure_ascii=False)\n",
    "        logger.info(f\"Saved {len(articles_data)} articles to {output_file}\")\n",
    "        print(f\"Scraping complete. Saved {len(articles_data)} articles to {output_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save JSON file: {e}\")\n",
    "    \n",
    "    return articles_data  # Return data for Jupyter notebook analysis\n",
    "\n",
    "# For Jupyter/IPython, run this cell\n",
    "articles_data = await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapes all the articles of times of india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from playwright.sync_api import sync_playwright\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to fetch article URLs from the entire site using Playwright\n",
    "def fetch_article_urls():\n",
    "    try:\n",
    "        with sync_playwright() as p:\n",
    "            # Launch headless browser\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            page = browser.new_page()\n",
    "\n",
    "            # Start from the homepage\n",
    "            homepage_url = 'https://timesofindia.indiatimes.com/'\n",
    "            logger.info(f\"Starting crawl from {homepage_url}\")\n",
    "            page.goto(homepage_url, wait_until=\"domcontentloaded\")\n",
    "\n",
    "            # Extract main section links (e.g., India, World, Business)\n",
    "            section_links = []\n",
    "            # Selector for section navigation (adjust based on page inspection)\n",
    "            nav_links = page.query_selector_all('nav a[href*=\"/india\"], nav a[href*=\"/world\"], nav a[href*=\"/business\"], nav a[href*=\"/sports\"], nav a[href*=\"/entertainment\"], nav a[href*=\"/lifestyle\"]')\n",
    "            for link in nav_links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href:\n",
    "                    if href.startswith('/'):\n",
    "                        href = 'https://timesofindia.indiatimes.com' + href\n",
    "                    if href not in section_links:\n",
    "                        section_links.append(href)\n",
    "            logger.info(f\"Found {len(section_links)} section links: {section_links}\")\n",
    "\n",
    "            # Collect article URLs from homepage and sections\n",
    "            article_urls = set()\n",
    "            \n",
    "            # Scrape homepage articles\n",
    "            logger.info(\"Fetching articles from homepage\")\n",
    "            page.wait_for_selector('a[href*=\"/articleshow/\"]', timeout=10000)\n",
    "            links = page.query_selector_all('a[href*=\"/articleshow/\"]')\n",
    "            for link in links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href:\n",
    "                    if href.startswith('/'):\n",
    "                        href = 'https://timesofindia.indiatimes.com' + href\n",
    "                    if re.match(r'.*/articleshow/\\d+\\.cms', href):\n",
    "                        article_urls.add(href)\n",
    "\n",
    "            # Scrape articles from each section\n",
    "            for section_url in section_links:\n",
    "                try:\n",
    "                    logger.info(f\"Fetching articles from section: {section_url}\")\n",
    "                    page.goto(section_url, wait_until=\"domcontentloaded\")\n",
    "                    page.wait_for_selector('a[href*=\"/articleshow/\"]', timeout=10000)\n",
    "\n",
    "                    # Handle pagination (fetch up to 3 pages per section to avoid excessive crawling)\n",
    "                    for page_num in range(3):\n",
    "                        links = page.query_selector_all('a[href*=\"/articleshow/\"]')\n",
    "                        for link in links:\n",
    "                            href = link.get_attribute('href')\n",
    "                            if href:\n",
    "                                if href.startswith('/'):\n",
    "                                    href = 'https://timesofindia.indiatimes.com' + href\n",
    "                                if re.match(r'.*/articleshow/\\d+\\.cms', href):\n",
    "                                    article_urls.add(href)\n",
    "                        \n",
    "                        # Check for \"Next\" button\n",
    "                        next_button = page.query_selector('a.next, a[rel=\"next\"]')\n",
    "                        if not next_button:\n",
    "                            break\n",
    "                        next_button.click()\n",
    "                        page.wait_for_load_state('domcontentloaded')\n",
    "                        time.sleep(1)  # Small delay for page load\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error fetching articles from section {section_url}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            browser.close()\n",
    "            article_urls = list(article_urls)\n",
    "            logger.info(f\"Found {len(article_urls)} unique article URLs\")\n",
    "            return article_urls\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching article URLs: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to process a single article using Playwright\n",
    "def process_article(article_url, index):\n",
    "    try:\n",
    "        with sync_playwright() as p:\n",
    "            # Launch headless browser\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            page = browser.new_page()\n",
    "\n",
    "            # Navigate to article page and wait for content to load\n",
    "            page.goto(article_url, wait_until=\"domcontentloaded\")\n",
    "            # Wait for article content to appear (adjust selector as needed)\n",
    "            page.wait_for_selector('h1', timeout=10000)\n",
    "\n",
    "            # Extract title\n",
    "            title_elem = page.query_selector('h1')\n",
    "            title = title_elem.inner_text().strip() if title_elem else \"No title found\"\n",
    "\n",
    "            # Extract publish date (adjust selector based on page structure)\n",
    "            date_elem = page.query_selector('div.as_byline > div > span')\n",
    "            publish_date = date_elem.inner_text().strip() if date_elem else None\n",
    "\n",
    "            # Extract full article text (adjust selector based on page structure)\n",
    "            article_body = page.query_selector_all('div._s30W > div._3WlLe')\n",
    "            article_text = ' '.join([elem.inner_text().strip() for elem in article_body]) if article_body else \"\"\n",
    "\n",
    "            # Extract author (adjust selector based on page structure)\n",
    "            author_elem = page.query_selector('div.as_byline > a')\n",
    "            author = author_elem.inner_text().strip() if author_elem else None\n",
    "\n",
    "            # Extract article-specific keywords/tags (e.g., from meta tags or visible tags)\n",
    "            article_keywords = []\n",
    "            # Try meta keywords\n",
    "            meta_keywords = page.query_selector('meta[name=\"keywords\"]')\n",
    "            if meta_keywords:\n",
    "                keywords_content = meta_keywords.get_attribute('content')\n",
    "                if keywords_content:\n",
    "                    article_keywords.extend([kw.strip() for kw in keywords_content.split(',')])\n",
    "            # Try visible tags (adjust selector based on page structure)\n",
    "            tag_elems = page.query_selector_all('div.tags a')\n",
    "            if tag_elems:\n",
    "                article_keywords.extend([elem.inner_text().strip() for elem in tag_elems])\n",
    "            # Remove duplicates and empty strings\n",
    "            article_keywords = list(set([kw for kw in article_keywords if kw]))\n",
    "\n",
    "            browser.close()\n",
    "\n",
    "            # Create article info\n",
    "            article_info = {\n",
    "                'title': title,\n",
    "                'url': article_url,\n",
    "                'publish_date': publish_date,\n",
    "                'author': author,\n",
    "                'article_keywords': article_keywords,\n",
    "                'full_text': article_text\n",
    "            }\n",
    "            return article_info, index\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error processing article {article_url}: {e}\")\n",
    "        return None, index\n",
    "\n",
    "# Main scraping process\n",
    "max_workers = 4  # Reduced workers due to Playwright's resource intensity\n",
    "articles_data = []\n",
    "try:\n",
    "    # Fetch all article URLs\n",
    "    article_urls = fetch_article_urls()\n",
    "    \n",
    "    if not article_urls:\n",
    "        logger.error(\"No articles found. Exiting.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Process articles in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all article processing tasks\n",
    "        future_to_article = {\n",
    "            executor.submit(process_article, url, i): url\n",
    "            for i, url in enumerate(article_urls, 1)\n",
    "        }\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_article):\n",
    "            article_info, index = future.result()\n",
    "            if article_info:\n",
    "                articles_data.append(article_info)\n",
    "                logger.info(f\"Processed article {index}: {article_info['title']}\")\n",
    "            # Small delay to avoid overwhelming the server\n",
    "            time.sleep(0.2)  # 200ms delay due to Playwright's heavier requests\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during parallel processing: {e}\")\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = 'toi_all_articles.json'\n",
    "try:\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles_data, f, indent=4, ensure_ascii=False)\n",
    "    logger.info(f\"Saved {len(articles_data)} articles to {output_file}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save JSON file: {e}\")\n",
    "\n",
    "print(f\"Scraping complete. Saved {len(articles_data)} articles to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape any articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from playwright.async_api import async_playwright\n",
    "import re\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load site configurations from sites.json\n",
    "try:\n",
    "    with open('sites.json', 'r', encoding='utf-8') as f:\n",
    "        sites = json.load(f)\n",
    "    logger.info(f\"Loaded {len(sites)} sites: {[site['name'] for site in sites]}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"sites.json not found. Please create the file with site configurations.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading sites.json: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Function to fetch article URLs from a site using Playwright Async API\n",
    "async def fetch_article_urls(site):\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            # Launch headless browser\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "\n",
    "            # Start from the homepage\n",
    "            homepage_url = site['homepage']\n",
    "            logger.info(f\"Starting crawl for {site['name']} from {homepage_url}\")\n",
    "            await page.goto(homepage_url, wait_until=\"domcontentloaded\")\n",
    "\n",
    "            # Extract section links\n",
    "            section_links = [homepage_url]  # Include homepage\n",
    "            section_selector = site.get('section_selector', 'nav a[href], header a[href]')\n",
    "            nav_links = await page.query_selector_all(section_selector)\n",
    "            for link in nav_links:\n",
    "                href = await link.get_attribute('href')\n",
    "                if href:\n",
    "                    if href.startswith('/'):\n",
    "                        href = site['base_url'] + href\n",
    "                    elif not href.startswith(('http://', 'https://')):\n",
    "                        href = site['base_url'].rstrip('/') + '/' + href.lstrip('/')\n",
    "                    if href not in section_links and site['base_url'] in href:\n",
    "                        section_links.append(href)\n",
    "            logger.info(f\"Found {len(section_links)} section links for {site['name']}\")\n",
    "\n",
    "            # Collect article URLs\n",
    "            article_urls = set()\n",
    "            article_selector = site.get('article_selector', 'a[href]')\n",
    "            article_regex = site.get('article_regex', r'.+/\\d+.*')\n",
    "\n",
    "            for section_url in section_links:\n",
    "                try:\n",
    "                    logger.info(f\"Fetching articles from {section_url}\")\n",
    "                    await page.goto(section_url, wait_until=\"domcontentloaded\")\n",
    "                    await page.wait_for_selector(article_selector, timeout=10000)\n",
    "\n",
    "                    # Handle pagination (up to 3 pages)\n",
    "                    for page_num in range(3):\n",
    "                        links = await page.query_selector_all(article_selector)\n",
    "                        for link in links:\n",
    "                            href = await link.get_attribute('href')\n",
    "                            if href:\n",
    "                                if href.startswith('/'):\n",
    "                                    href = site['base_url'] + href\n",
    "                                elif not href.startswith(('http://', 'https://')):\n",
    "                                    href = site['base_url'].rstrip('/') + '/' + href.lstrip('/')\n",
    "                                if re.match(article_regex, href) and href not in article_urls:\n",
    "                                    article_urls.add(href)\n",
    "\n",
    "                        # Check for \"Next\" button\n",
    "                        next_selector = site.get('next_selector', 'a.next, a[rel=\"next\"], a[aria-label*=\"next\"]')\n",
    "                        next_button = await page.query_selector(next_selector)\n",
    "                        if not next_button:\n",
    "                            break\n",
    "                        await next_button.click()\n",
    "                        await page.wait_for_load_state('domcontentloaded')\n",
    "                        await asyncio.sleep(1)  # Delay for page load\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error fetching articles from {section_url}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            await browser.close()\n",
    "            article_urls = list(article_urls)\n",
    "            logger.info(f\"Found {len(article_urls)} unique article URLs for {site['name']}\")\n",
    "            return article_urls, site['name']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching article URLs for {site['name']}: {e}\")\n",
    "        return [], site['name']\n",
    "\n",
    "# Function to process a single article using Playwright Async API\n",
    "async def process_article(article_url, site, index):\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            # Launch headless browser\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "\n",
    "            # Navigate to article page\n",
    "            await page.goto(article_url, wait_until=\"domcontentloaded\")\n",
    "            await page.wait_for_selector(site.get('title_selector', 'h1, h2, article h1'), timeout=10000)\n",
    "\n",
    "            # Extract title\n",
    "            title_elem = await page.query_selector(site.get('title_selector', 'h1, h2, article h1'))\n",
    "            title = await title_elem.inner_text() if title_elem else \"No title found\"\n",
    "            title = title.strip()\n",
    "\n",
    "            # Extract publish date\n",
    "            date_selector = site.get('date_selector', 'time, meta[name=\"publish-date\"], div.date, span.date')\n",
    "            date_elem = await page.query_selector(date_selector)\n",
    "            publish_date = await date_elem.inner_text() if date_elem else None\n",
    "            if not publish_date and date_elem:\n",
    "                publish_date = await date_elem.get_attribute('content') or await date_elem.get_attribute('datetime')\n",
    "            if publish_date:\n",
    "                publish_date = publish_date.strip()\n",
    "\n",
    "            # Extract full article text\n",
    "            text_selector = site.get('text_selector', 'article, div.article-content, div.story-content, div.content')\n",
    "            article_body = await page.query_selector_all(text_selector + ' p')\n",
    "            article_text = ' '.join([await elem.inner_text() for elem in article_body]).strip() if article_body else \"\"\n",
    "\n",
    "            # Extract author\n",
    "            author_selector = site.get('author_selector', 'span.author, div.byline, a[rel=\"author\"], meta[name=\"author\"]')\n",
    "            author_elem = await page.query_selector(author_selector)\n",
    "            author = await author_elem.inner_text() if author_elem else None\n",
    "            if not author and author_elem:\n",
    "                author = await author_elem.get_attribute('content')\n",
    "            if author:\n",
    "                author = author.strip()\n",
    "\n",
    "            # Extract article-specific keywords/tags\n",
    "            article_keywords = []\n",
    "            # Meta keywords\n",
    "            meta_keywords = await page.query_selector('meta[name=\"keywords\"], meta[property=\"article:tag\"]')\n",
    "            if meta_keywords:\n",
    "                keywords_content = await meta_keywords.get_attribute('content')\n",
    "                if keywords_content:\n",
    "                    article_keywords.extend([kw.strip() for kw in keywords_content.split(',')])\n",
    "            # Visible tags\n",
    "            tag_selector = site.get('tag_selector', 'div.tags a, ul.tags li, a.tag')\n",
    "            tag_elems = await page.query_selector_all(tag_selector)\n",
    "            if tag_elems:\n",
    "                article_keywords.extend([await elem.inner_text() for elem in tag_elems])\n",
    "            # Remove duplicates and empty strings\n",
    "            article_keywords = list(set([kw.strip() for kw in article_keywords if kw]))\n",
    "\n",
    "            await browser.close()\n",
    "\n",
    "            # Create article info\n",
    "            article_info = {\n",
    "                'site': site['name'],\n",
    "                'title': title,\n",
    "                'url': article_url,\n",
    "                'publish_date': publish_date,\n",
    "                'author': author,\n",
    "                'article_keywords': article_keywords,\n",
    "                'full_text': article_text\n",
    "            }\n",
    "            return article_info, index\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error processing article {article_url} from {site['name']}: {e}\")\n",
    "        return None, index\n",
    "\n",
    "# Wrapper to run async function in sync context for ThreadPoolExecutor\n",
    "def run_async_fetch(site):\n",
    "    return asyncio.run(fetch_article_urls(site))\n",
    "\n",
    "def run_async_process(article_url, site, index):\n",
    "    return asyncio.run(process_article(article_url, site, index))\n",
    "\n",
    "# Main scraping process\n",
    "max_workers = 4  # Adjust based on system resources\n",
    "articles_data = []\n",
    "try:\n",
    "    for site in sites:\n",
    "        # Fetch article URLs for the site\n",
    "        article_urls, site_name = run_async_fetch(site)\n",
    "        \n",
    "        if not article_urls:\n",
    "            logger.warning(f\"No articles found for {site_name}\")\n",
    "            continue\n",
    "\n",
    "        # Process articles in parallel\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_article = {\n",
    "                executor.submit(run_async_process, url, site, i): url\n",
    "                for i, url in enumerate(article_urls, 1)\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_article):\n",
    "                article_info, index = future.result()\n",
    "                if article_info:\n",
    "                    articles_data.append(article_info)\n",
    "                    logger.info(f\"Processed article {index} from {site_name}: {article_info['title']}\")\n",
    "                time.sleep(0.2)  # Delay to avoid overwhelming servers\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during parallel processing: {e}\")\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = 'news_articles.json'\n",
    "try:\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles_data, f, indent=4, ensure_ascii=False)\n",
    "    logger.info(f\"Saved {len(articles_data)} articles to {output_file}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save JSON file: {e}\")\n",
    "\n",
    "print(f\"Scraping complete. Saved {len(articles_data)} articles to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To convert to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavi/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-1.0.7-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: sentence-transformers in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (4.1.0)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic>=1.9 (from chromadb)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp39-cp39-macosx_10_9_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi==0.115.9 (from chromadb)\n",
      "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from chromadb) (4.13.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.19.2-cp39-cp39-macosx_11_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from chromadb) (0.21.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from chromadb) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: importlib-resources in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from chromadb) (1.69.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.15.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (16 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.18-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Collecting httpx>=0.27.0 (from chromadb)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting jsonschema>=4.19.0 (from chromadb)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from build>=1.0.3->chromadb) (8.5.0)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb)\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting anyio (from httpx>=0.27.0->chromadb)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27.0->chromadb)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: requests in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb)\n",
      "  Downloading rpds_py-0.24.0-cp39-cp39-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: protobuf in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro>=1.5.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=1.9->chromadb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=1.9->chromadb)\n",
      "  Downloading pydantic_core-2.33.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=1.9->chromadb)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: networkx in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.5-cp39-cp39-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-15.0.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from importlib-resources->chromadb) (3.21.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pranavi/Library/Python/3.9/lib/python/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading chromadb-1.0.7-cp39-abi3-macosx_10_12_x86_64.whl (17.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp39-cp39-macosx_10_9_x86_64.whl (195 kB)\n",
      "Downloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl (498 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.1.0-cp39-cp39-macosx_10_9_x86_64.whl (40 kB)\n",
      "Downloading onnxruntime-1.19.2-cp39-cp39-macosx_11_0_universal2.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.32.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl (188 kB)\n",
      "Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.32.1-py3-none-any.whl (118 kB)\n",
      "Downloading orjson-3.10.18-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp39-cp39-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typer-0.15.3-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading httptools-0.6.4-cp39-cp39-macosx_10_9_universal2.whl (201 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.24.0-cp39-cp39-macosx_10_12_x86_64.whl (378 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading uvloop-0.21.0-cp39-cp39-macosx_10_9_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.9/800.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp39-cp39-macosx_10_12_x86_64.whl (405 kB)\n",
      "Downloading websockets-15.0.1-cp39-cp39-macosx_10_9_x86_64.whl (173 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading wrapt-1.17.2-cp39-cp39-macosx_10_9_x86_64.whl (38 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=0ed53882b352ade1997e08f96eeb394258b522a5f5f20e4233b0555880937b97\n",
      "  Stored in directory: /Users/pranavi/Library/Caches/pip/wheels/f7/02/64/d541eac67ec459309d1fb19e727f58ecf7ffb4a8bf42d4cfe5\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, flatbuffers, durationpy, wrapt, websockets, uvloop, typing-inspection, tomli, tenacity, shellingham, rpds-py, python-dotenv, pyproject_hooks, pydantic-core, pyasn1, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, oauthlib, mmh3, mdurl, humanfriendly, httptools, h11, googleapis-common-protos, distro, chroma-hnswlib, bcrypt, backoff, asgiref, anyio, annotated-types, watchfiles, uvicorn, starlette, rsa, requests-oauthlib, referencing, pydantic, pyasn1-modules, posthog, opentelemetry-exporter-otlp-proto-common, markdown-it-py, httpcore, deprecated, coloredlogs, build, rich, opentelemetry-api, onnxruntime, jsonschema-specifications, httpx, google-auth, fastapi, typer, opentelemetry-semantic-conventions, kubernetes, jsonschema, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.7 coloredlogs-15.0.1 deprecated-1.2.18 distro-1.9.0 durationpy-0.9 fastapi-0.115.9 flatbuffers-25.2.10 google-auth-2.39.0 googleapis-common-protos-1.70.0 h11-0.16.0 httpcore-1.0.9 httptools-0.6.4 httpx-0.28.1 humanfriendly-10.0 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 kubernetes-32.0.1 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.1.0 oauthlib-3.2.2 onnxruntime-1.19.2 opentelemetry-api-1.32.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-sdk-1.32.1 opentelemetry-semantic-conventions-0.53b1 opentelemetry-util-http-0.53b1 orjson-3.10.18 overrides-7.7.0 posthog-4.0.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.4 pydantic-core-2.33.2 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 referencing-0.36.2 requests-oauthlib-2.0.0 rich-14.0.0 rpds-py-0.24.0 rsa-4.9.1 shellingham-1.5.4 starlette-0.45.3 tenacity-9.1.2 tomli-2.2.1 typer-0.15.3 typing-inspection-0.4.0 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5 websockets-15.0.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 18:39:05,421 - INFO - Loaded 13 articles from toi_articles_all.json\n",
      "2025-05-01 18:39:05,449 - INFO - Use pytorch device_name: cpu\n",
      "2025-05-01 18:39:05,451 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-05-01 18:39:11,370 - INFO - Initialized SentenceTransformer model: all-MiniLM-L6-v2\n",
      "2025-05-01 18:39:11,413 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-05-01 18:39:12,689 - INFO - Initialized Chroma DB collection: toi_articles\n",
      "2025-05-01 18:39:12,995 - INFO - Prepared article 1: Infosys Eyes Experienced Tech Workers across 40+ Skill Sets\n",
      "2025-05-01 18:39:13,045 - INFO - Prepared article 2: Instagram’s head Adam Mosseri dances with influencers, gets a Bollywood welcome from Ranveer and Deepika in Mumbai\n",
      "2025-05-01 18:39:13,085 - INFO - Prepared article 3: Yolanthe OTT Release Date: When to watch Dutch reality show peeking into Yolanthe Cabau’s life\n",
      "2025-05-01 18:39:13,124 - INFO - Prepared article 4: Met Gala 2025: Meet Anna Wintour, Vogue’s editor-in-chief and fashion’s most powerful force\n",
      "2025-05-01 18:39:13,164 - INFO - Prepared article 5: Hania Aamir’s ‘message to PM Modi’ over Instagram ban: Is she blaming Pakistani army for Pahalgam attack?\n",
      "2025-05-01 18:39:13,205 - INFO - Prepared article 6: Another global pandemic fears rise as H5N1 bird flu spreads across US farms\n",
      "2025-05-01 18:39:13,245 - INFO - Prepared article 7: Rohit Sharma's net worth: Inside Hitman's net worth, income, IPL & BCCI salary, house, cars on his 38th birthday\n",
      "2025-05-01 18:39:13,287 - INFO - Prepared article 8: Mark Carney's net worth: How rich is the 24th PM of Canada? A look at his career, family\n",
      "2025-05-01 18:39:13,320 - INFO - Prepared article 9: Smartphone Under 10K: Mobile Phones under 10000\n",
      "2025-05-01 18:39:13,358 - INFO - Prepared article 10: Smartphones under 20K: Mobile Phones under 20000\n",
      "2025-05-01 18:39:13,398 - INFO - Prepared article 11: Laptops under 40000 Online in India – Gadgets Now\n",
      "2025-05-01 18:39:13,432 - INFO - Prepared article 12: AC Under 30000 Online in India\n",
      "2025-05-01 18:39:13,468 - INFO - Prepared article 13: Best Refrigerators under 10000 Online in India\n",
      "2025-05-01 18:39:13,474 - ERROR - Failed to add articles to Chroma DB: Expected metadata value to be a str, int, float or bool, got None which is a NoneType in add.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.24it/s]\n",
      "2025-05-01 18:39:13,500 - INFO - Query results:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed. Stored 13 articles in Chroma DB.\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the JSON file\n",
    "input_file = 'toi_articles_all.json'\n",
    "try:\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        articles_data = json.load(f)\n",
    "    logger.info(f\"Loaded {len(articles_data)} articles from {input_file}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"{input_file} not found. Please ensure the file exists.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading {input_file}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "try:\n",
    "    embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    logger.info(\"Initialized SentenceTransformer model: all-MiniLM-L6-v2\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize SentenceTransformer model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize Chroma DB client\n",
    "try:\n",
    "    chroma_client = chromadb.PersistentClient(\n",
    "        path=\"./chroma_db\",\n",
    "        settings=Settings()\n",
    "    )\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"toi_articles\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    logger.info(\"Initialized Chroma DB collection: toi_articles\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize Chroma DB: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Prepare data for Chroma DB\n",
    "documents = []\n",
    "embeddings = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "for i, article in enumerate(articles_data, 1):\n",
    "    try:\n",
    "        # Prepare text for embedding\n",
    "        document = f\"{article['title']} {article['summary']}\"\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = embed_model.encode(document, show_progress_bar=False).tolist()\n",
    "        \n",
    "        # Prepare metadata\n",
    "        metadata = {\n",
    "            'title': article['title'],\n",
    "            'url': article['url'],\n",
    "            'publish_date': article['publish_date'],\n",
    "            'keywords': \",\".join(article['keywords'])\n",
    "        }\n",
    "        article_id = f\"article_{i}\"\n",
    "\n",
    "        documents.append(document)\n",
    "        embeddings.append(embedding)\n",
    "        metadatas.append(metadata)\n",
    "        ids.append(article_id)\n",
    "\n",
    "        logger.info(f\"Prepared article {i}: {article['title']}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error preparing article {article.get('title', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Add data to Chroma DB\n",
    "try:\n",
    "    if documents:\n",
    "        collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        logger.info(f\"Added {len(ids)} articles to Chroma DB\")\n",
    "    else:\n",
    "        logger.warning(\"No articles to add to Chroma DB\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to add articles to Chroma DB: {e}\")\n",
    "\n",
    "# Example query\n",
    "try:\n",
    "    query_text = \"education policy in India\"\n",
    "    query_embedding = embed_model.encode(query_text).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3\n",
    "    )\n",
    "    logger.info(\"Query results:\")\n",
    "    for i, (id_, dist, meta, doc) in enumerate(zip(\n",
    "        results['ids'][0], results['distances'][0], results['metadatas'][0], results['documents'][0]\n",
    "    ), 1):\n",
    "        logger.info(f\"Result {i}: {meta['title']} (Distance: {dist:.4f})\")\n",
    "        logger.info(f\"URL: {meta['url']}\")\n",
    "        logger.info(f\"Keywords: {meta['keywords']}\")\n",
    "        logger.info(f\"Summary: {doc[:200]}...\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error querying Chroma DB: {e}\")\n",
    "\n",
    "print(f\"Completed. Stored {len(ids)} articles in Chroma DB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 18:43:18,345 - INFO - Connected to Chroma DB\n",
      "2025-05-01 18:43:18,347 - INFO - Accessed collection: toi_articles\n",
      "2025-05-01 18:43:18,349 - INFO - Total articles in collection: 0\n",
      "2025-05-01 18:43:18,351 - WARNING - No articles found in the collection\n",
      "2025-05-01 18:43:18,353 - WARNING - No article found with ID: article_1\n",
      "2025-05-01 18:43:18,356 - INFO - Use pytorch device_name: cpu\n",
      "2025-05-01 18:43:18,357 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-05-01 18:43:21,440 - INFO - Initialized SentenceTransformer model: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.29it/s]\n",
      "2025-05-01 18:43:27,174 - INFO - Query results for: 'Instagram’s head Adam Mosseri'\n",
      "2025-05-01 18:43:27,176 - INFO - All collections in Chroma DB:\n",
      "2025-05-01 18:43:27,178 - INFO -   Name: toi_articles, Metadata: {'hnsw:space': 'cosine'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Chroma DB client\n",
    "try:\n",
    "    chroma_client = chromadb.PersistentClient(\n",
    "        path=\"./chroma_db\",\n",
    "        settings=Settings()\n",
    "    )\n",
    "    logger.info(\"Connected to Chroma DB\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to connect to Chroma DB: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Access the collection\n",
    "collection_name = \"toi_articles\"\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "    logger.info(f\"Accessed collection: {collection_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to access collection {collection_name}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 1. Check total number of items\n",
    "try:\n",
    "    count = collection.count()\n",
    "    logger.info(f\"Total articles in collection: {count}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to count items: {e}\")\n",
    "\n",
    "# 2. List all articles (with pagination to avoid memory issues)\n",
    "try:\n",
    "    if count > 0:\n",
    "        logger.info(\"Listing all articles:\")\n",
    "        batch_size = 100  # Adjust based on memory constraints\n",
    "        for offset in range(0, count, batch_size):\n",
    "            items = collection.get(\n",
    "                include=['documents', 'metadatas'],\n",
    "                limit=batch_size,\n",
    "                offset=offset\n",
    "            )\n",
    "            for i, (id_, doc, meta) in enumerate(zip(items['ids'], items['documents'], items['metadatas']), offset + 1):\n",
    "                logger.info(f\"Article {i}:\")\n",
    "                logger.info(f\"  ID: {id_}\")\n",
    "                logger.info(f\"  Title: {meta['title']}\")\n",
    "                logger.info(f\"  URL: {meta['url']}\")\n",
    "                logger.info(f\"  Keywords: {meta['keywords']}\")\n",
    "                logger.info(f\"  Summary: {doc[:200]}...\")\n",
    "                logger.info(f\"  Publish Date: {meta['publish_date']}\\n\")\n",
    "    else:\n",
    "        logger.warning(\"No articles found in the collection\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to list articles: {e}\")\n",
    "\n",
    "# 3. Retrieve specific article by ID (example)\n",
    "try:\n",
    "    sample_id = \"article_1\"  # Replace with a known ID\n",
    "    item = collection.get(ids=[sample_id], include=['documents', 'metadatas'])\n",
    "    if item['ids']:\n",
    "        logger.info(f\"Retrieved article with ID: {sample_id}\")\n",
    "        logger.info(f\"  Title: {item['metadatas'][0]['title']}\")\n",
    "        logger.info(f\"  URL: {item['metadatas'][0]['url']}\")\n",
    "        logger.info(f\"  Keywords: {item['metadatas'][0]['keywords']}\")\n",
    "        logger.info(f\"  Summary: {item['documents'][0][:200]}...\")\n",
    "        logger.info(f\"  Publish Date: {item['metadatas'][0]['publish_date']}\\n\")\n",
    "    else:\n",
    "        logger.warning(f\"No article found with ID: {sample_id}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to retrieve article by ID: {e}\")\n",
    "\n",
    "# 4. Perform a custom query\n",
    "try:\n",
    "    # Initialize SentenceTransformer for query embedding\n",
    "    embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    logger.info(\"Initialized SentenceTransformer model: all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Custom query (modify this to search for different topics)\n",
    "    query_text = input(\"Enter a query (e.g., 'education policy in India') or press Enter for default: \") or \"education policy in India\"\n",
    "    query_embedding = embed_model.encode(query_text).tolist()\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=5  # Return up to 5 results\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Query results for: '{query_text}'\")\n",
    "    for i, (id_, dist, meta, doc) in enumerate(zip(\n",
    "        results['ids'][0], results['distances'][0], results['metadatas'][0], results['documents'][0]\n",
    "    ), 1):\n",
    "        logger.info(f\"Result {i}:\")\n",
    "        logger.info(f\"  ID: {id_}\")\n",
    "        logger.info(f\"  Title: {meta['title']}\")\n",
    "        logger.info(f\"  URL: {meta['url']}\")\n",
    "        logger.info(f\"  Keywords: {meta['keywords']}\")\n",
    "        logger.info(f\"  Summary: {doc[:200]}...\")\n",
    "        logger.info(f\"  Distance: {dist:.4f}\\n\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to perform query: {e}\")\n",
    "\n",
    "# 5. List all collections (for context)\n",
    "try:\n",
    "    collections = chroma_client.list_collections()\n",
    "    logger.info(\"All collections in Chroma DB:\")\n",
    "    for coll in collections:\n",
    "        logger.info(f\"  Name: {coll.name}, Metadata: {coll.metadata}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to list collections: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
